{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "IS_DEBUG_MODE = False\n",
    "CLEARANCE = 190\n",
    "\n",
    "# Define the path\n",
    "DIR_PATHS = [\n",
    "    r'analysis_path'\n",
    "]\n",
    "OUTPUT_FILE_NAME = 'Report_Tables.csv'\n",
    "\n",
    "# Define the search strings in a dictionary\n",
    "SEARCH_PATTERNS = {\n",
    "    'ronja.dat': '*rj.dat',\n",
    "    'ph_riser.ARS': '*ph10.ARS'\n",
    "}\n",
    "\n",
    "def find_files(directory_path, search_patterns):\n",
    "    file_data = []\n",
    "\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for _, pattern in search_patterns.items():\n",
    "            for filename in fnmatch.filter(files, pattern):\n",
    "                file_data.append({\n",
    "                    'Search String': pattern,\n",
    "                    'File Name': filename,\n",
    "                    'Path': os.path.join(root, filename)\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(file_data[:2] if IS_DEBUG_MODE else file_data)\n",
    "\n",
    "def extract_lines(file_path, search_string, start_lines, no_lines):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        if search_string in line:\n",
    "            return lines[i+start_lines:i+no_lines]\n",
    "    return []\n",
    "\n",
    "def find_string_in_file(file_path, search_string):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return any(search_string in line for line in file)\n",
    "\n",
    "def extract_lines_and_find_strings(file_path, search_strings):\n",
    "    chunk_size = 20 * 100000  # 20 MB, approx. > 160k lines\n",
    "    lines = []\n",
    "    found_strings = {search_string: False for search_string in search_strings}\n",
    "    extracted_lines = {search_string: [] for search_string in search_strings}\n",
    "    with open(file_path, 'r') as file:\n",
    "        while True:\n",
    "            chunk = file.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            chunk_lines = chunk.splitlines()\n",
    "            lines.extend(chunk_lines)\n",
    "            for search_string in search_strings:\n",
    "                if not found_strings[search_string] and any(search_string in line for line in chunk_lines):\n",
    "                    found_strings[search_string] = True\n",
    "    for search_string, (start_lines, no_lines) in search_strings.items():\n",
    "        for i, line in enumerate(lines):\n",
    "            if search_string in line:\n",
    "                extracted_lines[search_string] = lines[i+start_lines:i+no_lines]\n",
    "                break\n",
    "    return extracted_lines, found_strings\n",
    "\n",
    "\n",
    "def analyze_rj_dat(file_path):\n",
    "    search_strings = {\n",
    "        'S H I P   I M P A C T   R E S U L T S': (5, 6), # start from 5th line and take 6 lines from the line with found search string\n",
    "        'Final time step reached.': (0, 0),\n",
    "        'Stop code for exeeded ult. limit strain': (0, 1)\n",
    "    }\n",
    "    extracted_lines, found_strings = extract_lines_and_find_strings(file_path, search_strings)\n",
    "    lines = extracted_lines['S H I P   I M P A C T   R E S U L T S']\n",
    "    is_final_time_step_reached = found_strings['Final time step reached.']\n",
    "    line_stop_code = extracted_lines['Stop code for exeeded ult. limit strain']\n",
    "\n",
    "    stop_code = int(line_stop_code[0].split()[-1]) if line_stop_code else None\n",
    "    \n",
    "    data = [re.split(r'\\s+|(?<=\\d)-', line.strip()) for line in lines] # split by space or if counter no space but negative sign\n",
    "\n",
    "    column_names = ['Node name', 'spring', 'beam 1', 'beam 2', 'Load case', 'Dent code', 'Force (kN)', \n",
    "                    'Deformations struct (m)', 'Deformations dent (m)', 'Deformations ship (m)', \n",
    "                    'Ship impact energies struct (kNm)', 'Ship impact energies dent (kNm)', \n",
    "                    'Ship impact energies ship (kNm)', 'Ship impact energies total (kNm)']\n",
    "    \n",
    "    if data: # exception for empty data, for instance the file is empty\n",
    "        if len(data[0]) != 14: # check if data has 14 columns, had exception when it was not\n",
    "            raise ValueError(f\"Data has {len(data[0])} columns, expected 14. Data: \\n{data}\")\n",
    "    else:\n",
    "        raise ValueError(\"No keyword 'Ship Impact Results' found in the file: \", file_path)\n",
    "\n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "    df['Final time step reached'] = 'YES' if is_final_time_step_reached else '-'\n",
    "    df['Stop Code'] = stop_code\n",
    "    \n",
    "    print(\"Ronja file analysed: \", file_path)\n",
    "    return add_file_info(df, file_path)\n",
    "\n",
    "def analyze_ph_ars(file_path):\n",
    "    df = pd.read_csv(file_path, header=6)\n",
    "    df = df[df['!NODDIS'] != '#EOF']  \n",
    "    return add_file_info(df, file_path)\n",
    "\n",
    "def add_file_info(df, file_path):\n",
    "    filename_parts = file_path.split('_')\n",
    "    rj_mod1 = filename_parts[-3]\n",
    "    rj_mod2 = filename_parts[-2]\n",
    "\n",
    "    df['File Path'] = file_path.replace('R:\\\\', '\\\\\\\\mack2\\\\og\\\\')\n",
    "    df['Dir'] = rj_mod1\n",
    "    df['Subdir'] = rj_mod2\n",
    "    return df\n",
    "\n",
    "FUNCTION_MAP = {\n",
    "    SEARCH_PATTERNS['ronja.dat']: analyze_rj_dat,\n",
    "    SEARCH_PATTERNS['ph_riser.ARS']: analyze_ph_ars,\n",
    "}\n",
    "\n",
    "def apply_function(row):\n",
    "    if row['Search String'] in FUNCTION_MAP:\n",
    "        return FUNCTION_MAP[row['Search String']](row['Path'])\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    df = pd.concat([find_files(path, SEARCH_PATTERNS) for path in DIR_PATHS], ignore_index=True)\n",
    "    df['result'] = df.apply(apply_function, axis=1)\n",
    "\n",
    "    mask = df['Search String'].str.endswith(SEARCH_PATTERNS['ronja.dat'])\n",
    "    merged_df = pd.concat(df.loc[mask, 'result'].tolist(), ignore_index=True)\n",
    "    \n",
    "    mask = df['Search String'].str.endswith(SEARCH_PATTERNS['ph_riser.ARS'])\n",
    "    filtered_dfs = df.loc[mask, 'result']\n",
    "    \n",
    "    if not filtered_dfs.empty:\n",
    "        merged_df2 = pd.concat(filtered_dfs.tolist(), ignore_index=True)\n",
    "        merged_df2 = merged_df2.rename(columns={merged_df2.columns[1]: 'Node name'})\n",
    "        merged_df2['Riser Displacement'] = merged_df2.apply(lambda row: math.sqrt(row['X-displacement']**2 + row['Y-displacement']**2), axis=1)\n",
    "        merged_df2['File Path ARS'] = merged_df2['File Path']\n",
    "        merged_df = pd.merge(merged_df, merged_df2[['Node name', 'Dir', 'Subdir', 'Riser Displacement', 'File Path ARS']], \n",
    "                            on=['Dir', 'Subdir'], \n",
    "                            how='outer', \n",
    "                            suffixes=('', '_y'))\n",
    "\n",
    "        merged_df = merged_df[merged_df['Node name'] != merged_df['Node name_y']]\n",
    "        merged_df = merged_df.drop(columns=['Node name_y'])\n",
    "        \n",
    "    else:\n",
    "        merged_df['Riser Displacement'] = math.nan\n",
    "        merged_df['File Path ARS'] = \"ARS File by search key: '({})' not found in the directory structure.\".format(SEARCH_PATTERNS['ph_riser.ARS'])\n",
    "\n",
    "    merged_df = merged_df.sort_values(by=['Dir', 'Subdir', 'Node name', 'Stop Code'])\n",
    "    \n",
    "    merged_df = merged_df.drop_duplicates(subset='File Path', keep='last') # remove dublicates if exist (happens if have ph_riser.ARS in both code directiories, fix later)\n",
    "    merged_df = merged_df.reset_index(drop=True) # fix index\n",
    "    print(merged_df)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merged_df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb1_df = merged_df[['Dir', 'Subdir', 'Stop Code', 'Ship impact energies struct (kNm)', 'Ship impact energies dent (kNm)', 'Ship impact energies ship (kNm)', 'Ship impact energies total (kNm)', 'Deformations struct (m)', 'Deformations dent (m)', 'Final time step reached', 'File Path']]\n",
    "print(tb1_df)\n",
    "\n",
    "# Define the headers\n",
    "tb1_headers1 = ['Scenario', 'SubScenario', 'Stop code', 'Absorbed energy (kNm)', '', '', '', 'Impact deformations (m)', '', 'Final time-step reached', 'Links:']\n",
    "tb1_headers2 = ['', '', '', 'Struct.', 'Dent', 'Ship', 'Total', 'Struct.', 'Dent', '', '']\n",
    "\n",
    "# Filter rows where 'Stop Code' is 3\n",
    "filtered_df = merged_df[merged_df['Stop Code'] == 3]\n",
    "\n",
    "# Create a new DataFrame with the desired columns\n",
    "tb2_df = filtered_df[['Dir', 'Subdir', 'Deformations struct (m)', 'Riser Displacement', 'Deformations dent (m)']].copy()\n",
    "\n",
    "# Rename the columns\n",
    "tb2_df.columns = ['Scenario', 'SubScenario', 'Struct. displacement', 'Riser Displacement', 'Caisson dent']\n",
    "\n",
    "# Convert columns to numeric (from m to mm and round to 0 decimals)\n",
    "tb2_df['Struct. displacement'] = (pd.to_numeric(tb2_df['Struct. displacement'], errors='coerce') * 1000).round(0)\n",
    "tb2_df['Riser Displacement'] = (pd.to_numeric(tb2_df['Riser Displacement'], errors='coerce') * 1000).round(0)\n",
    "tb2_df['Caisson dent'] = (pd.to_numeric(tb2_df['Caisson dent'], errors='coerce') * 1000).round(0)\n",
    "\n",
    "# Add a new column with a fixed value\n",
    "tb2_df['Available clearance'] = CLEARANCE  # replace 100 with the desired value\n",
    "\n",
    "# Add a new column with a value calculated from previous columns\n",
    "tb2_df['Residual post-impact clearance'] = tb2_df['Available clearance'] - tb2_df['Struct. displacement'] + tb2_df['Riser Displacement'] - tb2_df['Caisson dent']\n",
    "\n",
    "tb2_df['Links:'] = merged_df['File Path ARS']\n",
    "tb2_df['Empty1'] = ''\n",
    "tb2_df['Empty2'] = ''\n",
    "tb2_df['Empty3'] = ''\n",
    "        \n",
    "# Reorder the columns\n",
    "tb2_df = tb2_df[['Scenario', 'SubScenario', 'Available clearance', 'Struct. displacement', 'Riser Displacement', 'Caisson dent', 'Residual post-impact clearance', 'Empty1', 'Empty2', 'Empty3', 'Links:']]\n",
    "\n",
    "\n",
    "print(tb2_df)\n",
    "\n",
    "# Define the headers\n",
    "tb2_headers1 = ['Scenario', 'SubScenario', 'Available hor.', 'Caisson hor.', 'Riser hor.', 'Caisson', 'Residual post-impact', '', '', '', 'Links:']\n",
    "tb2_headers2 = ['', '', 'clearance.', 'displacement.', 'displacement.', 'dent', 'clearance', '', '', '', '']\n",
    "tb2_headers3 = ['', '', '[mm].', '[mm].', '[mm].', '[mm]', '[mm]', '', '', '', '']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "PRINT_RESULTS = True\n",
    "\n",
    "if PRINT_RESULTS:\n",
    "    # Remove the old file if it exists\n",
    "    if os.path.exists(OUTPUT_FILE_NAME):\n",
    "        os.remove(OUTPUT_FILE_NAME)\n",
    "        \n",
    "    # Write tb1_headers1 and tb1_headers2 to a CSV file\n",
    "    with open(OUTPUT_FILE_NAME, 'w', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter=';')\n",
    "        writer.writerow(tb1_headers1)\n",
    "        writer.writerow(tb1_headers2)\n",
    "\n",
    "    # Append the first DataFrame to the CSV file\n",
    "    tb1_df.to_csv(OUTPUT_FILE_NAME, mode='a', index=False, sep=';', header=False)\n",
    "\n",
    "    # Write 3 empty rows and new table headers to the CSV file\n",
    "    with open(OUTPUT_FILE_NAME, 'a', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter=';')\n",
    "        writer.writerow([])\n",
    "        writer.writerow([])\n",
    "        writer.writerow([])\n",
    "        writer.writerow(tb2_headers1)\n",
    "        writer.writerow(tb2_headers2)\n",
    "        writer.writerow(tb2_headers3)\n",
    "\n",
    "    # Append the second DataFrame to the CSV file\n",
    "    tb2_df.to_csv(OUTPUT_FILE_NAME, mode='a', index=False, sep=';', header=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
